{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dabfea76-b93b-4efe-b6e4-0f0b678a936b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "!ls hyperliquid/utils/kafka_callbacks.py\n",
    "!ls hyperliquid/utils/rate_limiter.py\n",
    "!ls hyperliquid/utils/example_utils.py\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0dabe7b6-0174-4f4d-ac1e-0239efcdbe43",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "!pip install aiokafka\n",
    "!pip install kafka-python\n",
    "dbutils.library.restartPython()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "46707a82-8e7a-447f-aabe-d819e4d1386e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import asyncio\n",
    "from asyncio import run_coroutine_threadsafe\n",
    "import os\n",
    "import json\n",
    "from datetime import datetime\n",
    "from typing import Any\n",
    "\n",
    "from hyperliquid.utils import constants\n",
    "from hyperliquid.utils import rate_limiter\n",
    "from hyperliquid.utils import example_utils\n",
    "\n",
    "from kafka import KafkaAdminClient\n",
    "from kafka.admin import NewTopic\n",
    "from kafka.errors import TopicAlreadyExistsError\n",
    "\n",
    "# Import our Kafka callbacks\n",
    "from hyperliquid.utils.kafka_callbacks import (\n",
    "    ClickHouseFillKafka,\n",
    "    ClickHouseOrderKafka,\n",
    "    ClickHouseLiquidationsKafka,\n",
    "    ClickHouseLiquidationsDevKafka\n",
    ")\n",
    "\n",
    "import yaml\n",
    "import time\n",
    "# USE_DEV = True\n",
    "# start_time = int(time.time() * 1000)\n",
    "\n",
    "def load_addresses_from_config(path=\"config_whales.yml\") -> list[str]:\n",
    "    try:\n",
    "        with open(path, \"r\") as f:\n",
    "            data = yaml.safe_load(f)\n",
    "            print(f\"Loaded addresses from {path}: {data}\")\n",
    "            return data.get(\"addresses\", [])\n",
    "    except Exception as e:\n",
    "        print(f\"[ERROR] Failed to load addresses from {path}: {e}\")\n",
    "        return []\n",
    "\n",
    "\n",
    "# Set up your Kafka config\n",
    "KAFKA_SERVER = \"cthki8qfdq8asdnsm9gg.any.us-east-1.mpx.prd.cloud.redpanda.com:9092\"\n",
    "USERNAME = \"alexei.jobfinder@gmail.com\"\n",
    "PASSWORD = \"y0obC7dFiU3CJxcsCH4RwXtwEhaauf\"\n",
    "username = \"alexei.jobfinder@gmail.com\"\n",
    "password = 'y0obC7dFiU3CJxcsCH4RwXtwEhaauf'\n",
    "bootstrap_servers = 'cthki8qfdq8asdnsm9gg.any.us-east-1.mpx.prd.cloud.redpanda.com:9092'\n",
    "\n",
    "fill_kafka_cb = None\n",
    "order_kafka_cb = None\n",
    "liquidation_kafka_cb = None\n",
    "\n",
    "# # Instantiate the fill/order Kafka callbacks\n",
    "# fill_kafka_cb = ClickHouseFillKafka(\n",
    "#     bootstrap=KAFKA_SERVER,\n",
    "#     username=USERNAME,\n",
    "#     password=PASSWORD\n",
    "# )\n",
    "\n",
    "# order_kafka_cb = ClickHouseOrderKafka(\n",
    "#     bootstrap=KAFKA_SERVER,\n",
    "#     username=USERNAME,\n",
    "#     password=PASSWORD\n",
    "# )\n",
    "# if USE_DEV:\n",
    "#     liquidation_kafka_cb = ClickHouseLiquidationsDevKafka(\n",
    "#         bootstrap=KAFKA_SERVER,\n",
    "#         username=USERNAME,\n",
    "#         password=PASSWORD\n",
    "#     )\n",
    "# else:\n",
    "#     liquidation_kafka_cb = ClickHouseLiquidationsKafka(\n",
    "#         bootstrap=KAFKA_SERVER,\n",
    "#         username=USERNAME,\n",
    "#         password=PASSWORD\n",
    "#     )\n",
    "\n",
    "\n",
    "rate_limiter = rate_limiter.HyperliquidRateLimiter()\n",
    "event_loop = asyncio.get_event_loop()\n",
    "\n",
    "\n",
    "def ensure_topics(bootstrap_servers, username, password, sasl_mechanism=\"SCRAM-SHA-256\"):\n",
    "    admin = KafkaAdminClient(\n",
    "        bootstrap_servers=bootstrap_servers,\n",
    "        security_protocol=\"SASL_SSL\",\n",
    "        sasl_mechanism=sasl_mechanism,\n",
    "        sasl_plain_username=username,\n",
    "        sasl_plain_password=password,\n",
    "    )\n",
    "\n",
    "    topics_to_create = [\n",
    "        'orders',\n",
    "        'fills',\n",
    "        'liquidations_dev',\n",
    "    ]\n",
    "\n",
    "    existing_topics = admin.list_topics()\n",
    "\n",
    "    new_topics = []\n",
    "    for t in topics_to_create:\n",
    "        if t not in existing_topics:\n",
    "            new_topics.append(NewTopic(name=t, num_partitions=1, replication_factor=3))\n",
    "\n",
    "    if new_topics:\n",
    "        try:\n",
    "            admin.create_topics(new_topics=new_topics)\n",
    "            print(\"Created topics:\", [nt.name for nt in new_topics])\n",
    "        except TopicAlreadyExistsError:\n",
    "            print(\"Some topics already exist.\")\n",
    "    else:\n",
    "        print(\"All topics already exist\")\n",
    "\n",
    "    admin.close()\n",
    "\n",
    "\n",
    "\n",
    "# Notation reference for Hyperliquid v0 API (nonstandard; subject to change in v1):\n",
    "# Px   = Price\n",
    "# Sz   = Size (in units of coin, i.e., base currency)\n",
    "# Szi  = Signed size (positive for long, negative for short)\n",
    "# Ntl  = Notional (USD amount, Px * Sz)\n",
    "# Side = Side of trade/book: B = Bid = Buy, A = Ask = Short (aggressing side for trades)\n",
    "# Asset = Integer representing the asset being traded\n",
    "# Tif  = Time in force: GTC = Good 'til canceled, ALO = Add liquidity only, IOC = Immediate or cancel\n",
    "\n",
    "#     {\\\"coin\\\":\\\"BTC\\\",\\\"px\\\":\\\"79823.0\\\",\\\"sz\\\":\\\"0.01126\\\",\\\"side\\\":\\\"A\\\",\\\"time\\\":1743963674808,\\\"startPosition\\\":\\\"0.01126\\\",\\\"dir\\\":\\\"Close Long\\\",\\\"closedPnl\\\":\\\"-6.768386\\\",\\\"hash\\\":\\\"0x766b22d7a6c15f8f4fef0421080ab00203730023705d3dd217a1dbfe8f96a41e\\\",\\\"oid\\\":84299475887,\\\"crossed\\\":true,\\\"fee\\\":\\\"0.224701\\\",\\\"tid\\\":602065776560497,\\\"liquidation\\\":{\\\"liquidatedUser\\\":\\\"0xa44481a6454f4fd0899e261aa941323f2b11a09b\\\",\\\"markPx\\\":\\\"79840.0\\\",\\\"method\\\":\\\"market\\\"},\\\"feeToken\\\":\\\"USDC\\\"},\n",
    "\n",
    "# Notation reference for Hyperliquid v0 API (nonstandard; subject to change in v1):\n",
    "# Px   = Price\n",
    "# Sz   = Size (in units of coin, i.e., base currency)\n",
    "# Szi  = Signed size (positive for long, negative for short)\n",
    "# Ntl  = Notional (USD amount, Px * Sz)\n",
    "# Side = Side of trade/book: B = Bid = Buy, A = Ask = Short (aggressing side for trades)\n",
    "# Asset = Integer representing the asset being traded\n",
    "# Tif  = Time in force: GTC = Good 'til canceled, ALO = Add liquidity only, IOC = Immediate or cancel\n",
    "\"\"\"\n",
    "Processes user fill messages, enriches with timing metadata, and writes to Kafka.\n",
    "\n",
    "Timing fields added:\n",
    "- `timestamp`: The original event time from the exchange, in milliseconds.\n",
    "- `receipt_timestamp`: Wall-clock time (in seconds, float) when this event was handled by your system.\n",
    "- `loop_timestamp`: Monotonic time (in seconds, float) when the event loop resumed this coroutine.\n",
    "- `loop_delay_nanoseconds`: Delay (in nanoseconds) between wall-clock `receipt_timestamp` and event loop resume.\n",
    "    → Measures how much time passed between the coroutine being scheduled vs. when it was actually resumed.\n",
    "- `event_loop_delay_nanoseconds`: Delay (in nanoseconds) between exchange event time and the loop resume time.\n",
    "    → Measures network delay, buffering, and event propagation latency.\n",
    "\n",
    "These fields help debug event latency, monitor scheduling jitter, and quantify end-to-end processing delays.\n",
    "\"\"\"\n",
    "\n",
    "# async def handle_user_fills(msg, address):\n",
    "#     await rate_limiter.acquire_address_action(address)\n",
    "#     fills = msg.get(\"data\", {}).get(\"fills\", [])\n",
    "#     for f in fills:\n",
    "#         try:\n",
    "#             ts = f.get(\"time\")  # milliseconds since Unix epoch (exchange)\n",
    "#             now = time.time()  # seconds (float), wall-clock\n",
    "#             loop_timestamp_sec = asyncio.get_event_loop().time()  # seconds (float), monotonic loop time\n",
    "#             loop_timestamp_ns = int(loop_timestamp_sec * 1_000_000_000)  # convert monotonic loop timestamp to nanoseconds\n",
    "\n",
    "#             # Convert all timestamps to nanoseconds for delay calculations\n",
    "#             receipt_timestamp_ns = int(now * 1_000_000_000)  # wall-clock to ns\n",
    "#             ts_ns = int(ts * 1_000_000)  # exchange timestamp from ms to ns\n",
    "\n",
    "#             # Calculate delays\n",
    "#             loop_delay_nanoseconds = receipt_timestamp_ns - ts_ns  # time from exchange event to arrival at system (network latency approximation)\n",
    "#             event_loop_delay_nanoseconds = loop_timestamp_ns - ts_ns  # exchange timestamp to processing (full latency)\n",
    "\n",
    "#             # Extract liquidation info\n",
    "#             liquidation = f.get(\"liquidation\", {}) or {}\n",
    "#             is_liquidation = 1 if liquidation else 0\n",
    "#             liquidated_user = liquidation.get(\"liquidatedUser\") if is_liquidation else -1\n",
    "#             liquidation_method = liquidation.get(\"method\") if is_liquidation else None\n",
    "#             liquidation_mark_price = liquidation.get(\"markPx\") if is_liquidation else -1\n",
    "\n",
    "#             normalized_side = {\"B\": \"buy\", \"A\": \"sell\"}.get(f.get(\"side\"), f.get(\"side\"))\n",
    "\n",
    "#             fill_data = {\n",
    "#                 \"exchange\": \"hyperliquid\",\n",
    "#                 \"symbol\": f.get(\"coin\"),\n",
    "#                 \"side\": normalized_side,\n",
    "#                 \"price\": f.get(\"px\"),\n",
    "#                 \"amount\": f.get(\"sz\"),\n",
    "#                 \"fee\": f.get(\"fee\"),\n",
    "#                 \"id\": f.get(\"hash\"),\n",
    "#                 \"order_id\": f.get(\"oid\"),\n",
    "#                 \"liquidity\": f.get(\"crossed\"),\n",
    "#                 \"type\": f.get(\"dir\"),\n",
    "#                 \"account\": address,\n",
    "#                 \"timestamp\": ts,  # exchange timestamp, ms\n",
    "#                 \"receipt_timestamp\": now,  # wall-clock seconds\n",
    "#                 \"loop_timestamp\": loop_timestamp_ns,  # monotonic loop timestamp, ns\n",
    "#                 \"loop_delay_nanoseconds\": loop_delay_nanoseconds,\n",
    "#                 \"event_loop_delay_nanoseconds\": event_loop_delay_nanoseconds,\n",
    "#                 \"is_liquidation\": is_liquidation,\n",
    "#                 \"liquidated_user\": liquidated_user,\n",
    "#                 \"liquidation_method\": liquidation_method,\n",
    "#                 \"liquidation_mark_price\": liquidation_mark_price,\n",
    "#                 \"raw\": f,\n",
    "#                 \"raw_data\": f,\n",
    "#             }\n",
    "\n",
    "#             print(f\"[DEBUG] Fills data for address {address}: {fill_data}\")\n",
    "#             await fill_kafka_cb.write(fill_data)\n",
    "\n",
    "\"\"\"\n",
    "Processes user fill messages, enriches them with timing metadata, and writes them to Kafka.\n",
    "\n",
    "Timing fields added:\n",
    "\n",
    "timestamp: Exchange-generated event timestamp (Unix epoch, nanoseconds). Represents the original event time provided by the exchange.\n",
    "\n",
    "receipt_timestamp: Wall-clock timestamp when the event reached your system (Unix epoch, nanoseconds). Indicates when your local machine received and began processing the event.\n",
    "\n",
    "loop_timestamp: Monotonic timestamp at the moment the event loop resumed processing this coroutine (nanoseconds since event loop start). Used for measuring internal scheduling latency independent of the system clock.\n",
    "\n",
    "loop_delay_nanoseconds: Latency between coroutine scheduling and event loop processing start, measured using monotonic timestamps. Reflects internal event loop scheduling delay (e.g., coroutine queuing delays). Typically expected to be very short (microseconds to low milliseconds).\n",
    "\n",
    "event_loop_delay_nanoseconds: Network/event propagation latency from exchange timestamp to local system receipt timestamp. Reflects real-world latency including network delay and event buffering.\n",
    "\n",
    "Real-world verified example:\n",
    "\n",
    "{\n",
    "  \"timestamp\": 1744089252941000000,          // exchange event time (~year 2025)\n",
    "  \"receipt_timestamp\": 1744089253069349400,  // received ~128ms later\n",
    "  \"loop_timestamp\": 1625465627457,           // monotonic loop timestamp (~27 min uptime)\n",
    "  \"loop_delay_nanoseconds\": 3660,            // internal scheduling delay ~3.66µs\n",
    "  \"event_loop_delay_nanoseconds\": 128349376  // external network delay ~128ms\n",
    "}\n",
    "\n",
    "These fields facilitate:\n",
    "\n",
    "Debugging of latency issues.\n",
    "\n",
    "Monitoring scheduling jitter.\n",
    "\n",
    "Quantifying end-to-end event processing performance.\n",
    "\"\"\"\n",
    "async def handle_user_fills(msg, address):\n",
    "    await rate_limiter.acquire_address_action(address)\n",
    "\n",
    "    arrival_wall_sec = time.time()  # Wall clock time at arrival (seconds since epoch)\n",
    "    arrival_loop_monotonic_sec = asyncio.get_event_loop().time()  # Monotonic arrival (loop start based)\n",
    "\n",
    "    fills = msg.get(\"data\", {}).get(\"fills\", [])\n",
    "    for f in fills:\n",
    "        try:\n",
    "            ts_ms = f.get(\"time\")  # milliseconds since epoch from exchange\n",
    "            exchange_timestamp_ns = int(ts_ms * 1_000_000)  # ms → ns\n",
    "\n",
    "            processing_loop_monotonic_ns = int(asyncio.get_event_loop().time() * 1_000_000_000)  # monotonic ns\n",
    "            arrival_loop_monotonic_ns = int(arrival_loop_monotonic_sec * 1_000_000_000)  # monotonic arrival ns\n",
    "            receipt_timestamp_ns = int(arrival_wall_sec * 1_000_000_000)  # wall-clock arrival ns\n",
    "\n",
    "            # Correct delays:\n",
    "            loop_delay_ns = processing_loop_monotonic_ns - arrival_loop_monotonic_ns\n",
    "            event_loop_delay_ns = receipt_timestamp_ns - exchange_timestamp_ns  # pure epoch-based difference\n",
    "\n",
    "            liquidation = f.get(\"liquidation\", {}) or {}\n",
    "            is_liquidation = 1 if liquidation else 0\n",
    "            liquidated_user = liquidation.get(\"liquidatedUser\") if is_liquidation else -1\n",
    "            liquidation_method = liquidation.get(\"method\") if is_liquidation else None\n",
    "            liquidation_mark_price = liquidation.get(\"markPx\") if is_liquidation else -1\n",
    "\n",
    "            normalized_side = {\"B\": \"buy\", \"A\": \"sell\"}.get(f.get(\"side\"), f.get(\"side\"))\n",
    "\n",
    "            fill_data = {\n",
    "                \"exchange\": \"hyperliquid\",\n",
    "                \"symbol\": f.get(\"coin\"),\n",
    "                \"side\": normalized_side,\n",
    "                \"price\": f.get(\"px\"),\n",
    "                \"amount\": f.get(\"sz\"),\n",
    "                \"fee\": f.get(\"fee\"),\n",
    "                \"id\": f.get(\"hash\"),\n",
    "                \"order_id\": f.get(\"oid\"),\n",
    "                \"liquidity\": f.get(\"crossed\"),\n",
    "                \"type\": f.get(\"dir\"),\n",
    "                \"account\": address,\n",
    "\n",
    "                \"timestamp\": exchange_timestamp_ns,  # exchange time (epoch ns)\n",
    "                \"receipt_timestamp\": receipt_timestamp_ns,  # wall clock (epoch ns)\n",
    "                \"loop_timestamp\": processing_loop_monotonic_ns,  # monotonic ns\n",
    "\n",
    "                \"loop_delay_nanoseconds\": loop_delay_ns,  # monotonic delay\n",
    "                \"event_loop_delay_nanoseconds\": event_loop_delay_ns,  # wall-clock (network) latency\n",
    "\n",
    "                \"is_liquidation\": is_liquidation,\n",
    "                \"liquidated_user\": liquidated_user,\n",
    "                \"liquidation_method\": liquidation_method,\n",
    "                \"liquidation_mark_price\": liquidation_mark_price,\n",
    "                \"raw\": f,\n",
    "                \"raw_data\": f,\n",
    "            }\n",
    "\n",
    "            print(f\"[DEBUG] Fills data for address {address}: {fill_data}\")\n",
    "            await fill_kafka_cb.write(fill_data)\n",
    "\n",
    "            if is_liquidation:\n",
    "                liquidation_data = {\n",
    "                    \"exchange\": \"hyperliquid\",\n",
    "                    \"symbol\": f.get(\"coin\"),\n",
    "                    \"side\": normalized_side,\n",
    "                    \"quantity\": f.get(\"sz\"),\n",
    "                    \"price\": f.get(\"px\"),\n",
    "                    \"id\": f.get(\"hash\"),\n",
    "                    \"status\": f.get(\"dir\"),\n",
    "                    \"timestamp\": exchange_timestamp_ns,  # exchange time (epoch ns)\n",
    "                    \"receipt_timestamp\": receipt_timestamp_ns,  # wall clock (epoch ns)\n",
    "                    \"loop_timestamp\": processing_loop_monotonic_ns,  # monotonic ns\n",
    "\n",
    "                    \"loop_delay_nanoseconds\": loop_delay_ns,  # monotonic delay\n",
    "                    \"event_loop_delay_nanoseconds\": event_loop_delay_ns,  # wall-clock (network) latency\n",
    "                    \"is_liquidation\": is_liquidation,\n",
    "                    \"liquidated_user\": liquidated_user,\n",
    "                    \"liquidation_method\": liquidation_method,\n",
    "                    \"liquidation_mark_price\": liquidation_mark_price,\n",
    "                    \"raw\": f,\n",
    "                    \"raw_data\": f,\n",
    "                }\n",
    "                await liquidation_kafka_cb.write(liquidation_data)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"[WARN] Failed to process fill for address {address}: {e}\")\n",
    "# async def handle_user_fills(msg, address):\n",
    "#     await rate_limiter.acquire_address_action(address)\n",
    "#     fills = msg.get(\"data\", {}).get(\"fills\", [])\n",
    "#     for f in fills:\n",
    "#         try:\n",
    "#             ts = f.get(\"time\")  # Unit: milliseconds since Unix epoch. exchange time.\n",
    "#             now = time.time()  # Unit: seconds (float) since Unix epoch. wall clock time\n",
    "#             # loop_timestamp = asyncio.get_event_loop().time()  # Unit: seconds (float) since the event loop started (not related to Unix epoch).\n",
    "#             loop_timestamp = int(asyncio.get_event_loop().time() * 1_000_000_000) # Unit: nanosecondsseconds (float) since the event loop started (not related to Unix epoch).\n",
    " \n",
    "#             # # Compute delays in nanoseconds\n",
    "#             # loop_delay_ns = int((now - loop_timestamp) * 1_000_000_000)\n",
    "#             # event_loop_delay_ns = int((loop_timestamp - (ts / 1000)) * 1_000_000_000)\n",
    "\n",
    "#             # Extract liquidation info\n",
    "#             liquidation = f.get(\"liquidation\", {}) or {}\n",
    "#             is_liquidation = 1 if liquidation else 0\n",
    "#             liquidated_user = liquidation.get(\"liquidatedUser\") if is_liquidation else -1\n",
    "#             liquidation_method = liquidation.get(\"method\") if is_liquidation else None\n",
    "#             liquidation_mark_price = liquidation.get(\"markPx\") if is_liquidation else -1\n",
    "\n",
    "#             normalized_side = {\"B\": \"buy\", \"A\": \"sell\"}.get(f.get(\"side\"), f.get(\"side\"))\n",
    "\n",
    "#             fill_data = {\n",
    "#                 \"exchange\": \"hyperliquid\",\n",
    "#                 \"symbol\": f.get(\"coin\"),\n",
    "#                 \"side\": normalized_side,\n",
    "#                 \"price\": f.get(\"px\"),\n",
    "#                 \"amount\": f.get(\"sz\"),\n",
    "#                 \"fee\": f.get(\"fee\"),\n",
    "#                 \"id\": f.get(\"hash\"),\n",
    "#                 \"order_id\": f.get(\"oid\"),\n",
    "#                 \"liquidity\": f.get(\"crossed\"),\n",
    "#                 \"type\": f.get(\"dir\"),\n",
    "#                 \"account\": address,\n",
    "#                 \"timestamp\": ts,\n",
    "#                 \"receipt_timestamp\": now,\n",
    "#                 \"loop_timestamp\": loop_timestamp,\n",
    "#                 # \"loop_delay_nanoseconds\": loop_delay_ns,\n",
    "#                 # \"event_loop_delay_nanoseconds\": event_loop_delay_ns,\n",
    "#                 \"is_liquidation\": is_liquidation,\n",
    "#                 \"liquidated_user\": liquidated_user,\n",
    "#                 \"liquidation_method\": liquidation_method,\n",
    "#                 \"liquidation_mark_price\": liquidation_mark_price,\n",
    "#                 \"raw\": f,\n",
    "#                 \"raw_data\": f,\n",
    "#             }\n",
    "\n",
    "#             print(f\"[DEBUG] Fills data for address {address}: {fill_data}\")\n",
    "#             await fill_kafka_cb.write(fill_data)\n",
    "\n",
    "#             if is_liquidation:\n",
    "#                 liquidation_data = {\n",
    "#                     \"exchange\": \"hyperliquid\",\n",
    "#                     \"symbol\": f.get(\"coin\"),\n",
    "#                     \"side\": normalized_side,\n",
    "#                     \"quantity\": f.get(\"sz\"),\n",
    "#                     \"price\": f.get(\"px\"),\n",
    "#                     \"id\": f.get(\"hash\"),\n",
    "#                     \"status\": f.get(\"dir\"),\n",
    "#                     \"timestamp\": ts,\n",
    "#                     \"receipt_timestamp\": now,\n",
    "#                     \"loop_timestamp\": loop_timestamp,\n",
    "#                     # \"loop_delay_nanoseconds\": loop_delay_ns,\n",
    "#                     # \"event_loop_delay_nanoseconds\": event_loop_delay_ns,\n",
    "#                     \"is_liquidation\": is_liquidation,\n",
    "#                     \"liquidated_user\": liquidated_user,\n",
    "#                     \"liquidation_method\": liquidation_method,\n",
    "#                     \"liquidation_mark_price\": liquidation_mark_price,\n",
    "#                     \"raw\": f,\n",
    "#                     \"raw_data\": f,\n",
    "#                 }\n",
    "#                 await liquidation_kafka_cb.write(liquidation_data)\n",
    "\n",
    "#         except Exception as e:\n",
    "#             print(f\"[WARN] Failed to process fill for address {address}: {e}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "async def handle_order_updates(msg, address):\n",
    "    # Rate limit for each order update.\n",
    "    await rate_limiter.acquire_address_action(address)\n",
    "    # now = asyncio.get_event_loop().time()\n",
    "\n",
    "    arrival_wall_sec = time.time()  # Wall clock time at arrival (seconds since epoch)\n",
    "    arrival_loop_monotonic_sec = asyncio.get_event_loop().time()  # Monotonic arrival (loop \n",
    "\n",
    "    for update in msg.get(\"data\", []):\n",
    "        order = update.get(\"order\", {})\n",
    "        side = order.get(\"side\")\n",
    "        normalized_side = {\"B\": \"buy\", \"A\": \"sell\"}.get(side, side)\n",
    "        ts_ms = update.get(\"time\")  # milliseconds since epoch from exchange\n",
    "        exchange_timestamp_ns = int(ts_ms * 1_000_000)  # ms → ns\n",
    "\n",
    "        processing_loop_monotonic_ns = int(asyncio.get_event_loop().time() * 1_000_000_000)  # monotonic ns\n",
    "        arrival_loop_monotonic_ns = int(arrival_loop_monotonic_sec * 1_000_000_000)  # monotonic arrival ns\n",
    "        receipt_timestamp_ns = int(arrival_wall_sec * 1_000_000_000)  # wall-clock arrival ns\n",
    "\n",
    "        # Correct delays:\n",
    "        loop_delay_ns = processing_loop_monotonic_ns - arrival_loop_monotonic_ns\n",
    "        event_loop_delay_ns = receipt_timestamp_ns - exchange_timestamp_ns  # pure epoch-based difference\n",
    "\n",
    "        # Compute remaining amount if both origSz and sz are available\n",
    "        try:\n",
    "            orig_sz = float(order.get(\"origSz\", \"nan\"))\n",
    "            current_sz = float(order.get(\"sz\", \"nan\"))\n",
    "            remaining_amount = orig_sz - current_sz\n",
    "        except Exception:\n",
    "            remaining_amount = None  # fallback if values are missing or invalid\n",
    "\n",
    "        order_data = {\n",
    "            \"exchange\": \"hyperliquid\",\n",
    "            \"symbol\": order.get(\"coin\"),\n",
    "            \"id\": order.get(\"oid\"),\n",
    "            \"client_order_id\": order.get(\"cloid\"),\n",
    "            \"side\": normalized_side,\n",
    "            \"status\": update.get(\"status\"),\n",
    "            \"type\": \"limit\" if order.get(\"limitPx\") else \"unknown\",\n",
    "            \"price\": order.get(\"limitPx\"),\n",
    "            \"amount\": order.get(\"sz\"),\n",
    "            \"remaining_amount\": str(remaining_amount) if remaining_amount is not None else None,\n",
    "            \"account\": address,\n",
    "            \"timestamp\": exchange_timestamp_ns,  # exchange time (epoch ns)\n",
    "            \"receipt_timestamp\": receipt_timestamp_ns,  # wall clock (epoch ns)\n",
    "            \"loop_timestamp\": processing_loop_monotonic_ns,  # monotonic ns\n",
    "            \"loop_delay_nanoseconds\": loop_delay_ns,  # monotonic delay\n",
    "            \"event_loop_delay_nanoseconds\": event_loop_delay_ns,  # wall-clock (network) latency\n",
    "            # \"timestamp\": ts,\n",
    "            # \"receipt_timestamp\": now,\n",
    "            # \"loop_timestamp\": loop_timestamp,\n",
    "            # \"loop_delay_nanoseconds\": loop_delay_ns,\n",
    "            # \"event_loop_delay_nanoseconds\": event_loop_delay_ns,\n",
    "            \"raw\": order,\n",
    "            \"raw_data\": msg\n",
    "        }\n",
    "\n",
    "        print(f\"[DEBUG] order_data for address {address}: {order_data}\")\n",
    "        await order_kafka_cb.write(order_data)\n",
    "\n",
    "\n",
    "# async def backfill_fills(address: str, _start_time: int):\n",
    "#     _, info, _ = example_utils.setup(constants.MAINNET_API_URL)\n",
    "\n",
    "#     # 3-day window (in milliseconds)\n",
    "#     end_time = int(time.time() * 1000)\n",
    "#     start_time = end_time - 7 * 86400 * 1000  # 7 days ago\n",
    "\n",
    "#     seen_fill_ids: set[str] = set()\n",
    "\n",
    "#     try:\n",
    "#         fills = info.user_fills_by_time(address, start_time, end_time)\n",
    "#         for msg in fills:\n",
    "#             fill_id = msg.get(\"hash\")\n",
    "#             if fill_id in seen_fill_ids:\n",
    "#                 continue\n",
    "#             seen_fill_ids.add(fill_id)\n",
    "#             await handle_user_fills({\"data\": {\"fills\": [msg]}}, address)\n",
    "#             await asyncio.sleep(0.01)  # throttle per fill\n",
    "#     except Exception as e:\n",
    "#         print(f\"[ERROR] backfill_fills failed for {address}: {e}\")\n",
    "\n",
    "\n",
    "#\n",
    "# NEW FUNCTION:\n",
    "# Subscribe a *batch* of addresses under a single `info` (hence one websocket).\n",
    "#\n",
    "async def subscribe_batch_of_addresses(address_batch: list[str], include_backfill_fills: bool):\n",
    "    try:\n",
    "        # Create one websocket connection for these 10 addresses\n",
    "        _, info, _ = example_utils.setup(constants.MAINNET_API_URL)\n",
    "\n",
    "        # Subscribe each address on the same connection\n",
    "        for address in address_batch:\n",
    "            await rate_limiter.acquire_address_action(address)\n",
    "            # if include_backfill_fills:\n",
    "            #     await backfill_fills(address, start_time)\n",
    "\n",
    "            info.subscribe(\n",
    "                {\"type\": \"userFills\", \"user\": address},\n",
    "                lambda msg: run_coroutine_threadsafe(handle_user_fills(msg, address), event_loop)\n",
    "            )\n",
    "            # info.subscribe(\n",
    "            #     {\"type\": \"orderUpdates\", \"user\": address},\n",
    "            #     lambda msg: run_coroutine_threadsafe(handle_order_updates(msg, address), event_loop)\n",
    "            # )\n",
    "\n",
    "            print(f\"[{address}] Subscribed on a shared websocket. Listening...\")\n",
    "\n",
    "        # Keep the connection for this batch active forever\n",
    "        while True:\n",
    "            await asyncio.sleep(5)\n",
    "\n",
    "    except Exception as e:\n",
    "        # If anything fails, retry\n",
    "        print(f\"[ERROR] Batch subscription failed {address_batch}: {e}\")\n",
    "        await asyncio.sleep(3)\n",
    "        return await subscribe_batch_of_addresses(address_batch, include_backfill_fills)\n",
    "\n",
    "\n",
    "#\n",
    "# MAIN method:\n",
    "# - ensures topics exist\n",
    "# - loads addresses\n",
    "# - for every 10 addresses, launches one subscription task (one websocket).\n",
    "#\n",
    "\n",
    "# NOTE; NO BACKFILL TAKING UP SPACE IN THE SOCKET, NO ORDERS. PURELY NET NEW FILLS. \n",
    "# WORTH DOING 1 backfill\n",
    "\n",
    "# async def main():\n",
    "#     INCLUDE_BACKFILL_FILLS = False\n",
    "#     ensure_topics(bootstrap_servers, username, password)\n",
    "\n",
    "async def main():\n",
    "    global fill_kafka_cb, order_kafka_cb, liquidation_kafka_cb\n",
    "    USE_DEV = False\n",
    "    INCLUDE_BACKFILL_FILLS = False\n",
    "\n",
    "    ensure_topics(bootstrap_servers, username, password)\n",
    "\n",
    "    fill_kafka_cb = ClickHouseFillKafka(\n",
    "        bootstrap=KAFKA_SERVER,\n",
    "        username=USERNAME,\n",
    "        password=PASSWORD\n",
    "    )\n",
    "\n",
    "    order_kafka_cb = ClickHouseOrderKafka(\n",
    "        bootstrap=KAFKA_SERVER,\n",
    "        username=USERNAME,\n",
    "        password=PASSWORD\n",
    "    )\n",
    "\n",
    "    if USE_DEV:\n",
    "        liquidation_kafka_cb = ClickHouseLiquidationsDevKafka(\n",
    "            bootstrap=KAFKA_SERVER,\n",
    "            username=USERNAME,\n",
    "            password=PASSWORD\n",
    "        )\n",
    "    else:\n",
    "        liquidation_kafka_cb = ClickHouseLiquidationsKafka(\n",
    "            bootstrap=KAFKA_SERVER,\n",
    "            username=USERNAME,\n",
    "            password=PASSWORD\n",
    "        )\n",
    "\n",
    "    addresses = load_addresses_from_config()\n",
    "\n",
    "    BATCH_SIZE = 10\n",
    "    tasks = []  # We'll collect one task per batch, so they all run concurrently.\n",
    "\n",
    "    for i in range(0, len(addresses), BATCH_SIZE):\n",
    "        batch = addresses[i : i + BATCH_SIZE]\n",
    "        print(f\"[INFO] Creating a subscription task for addresses {batch}\")\n",
    "        t = asyncio.create_task(subscribe_batch_of_addresses(batch, INCLUDE_BACKFILL_FILLS))\n",
    "        tasks.append(t)\n",
    "\n",
    "    # Keep the main alive while all batch tasks run forever\n",
    "    await asyncio.gather(*tasks)\n",
    "\n",
    "# Uncomment if you want to run directly:\n",
    "# if __name__ == \"__main__\":\n",
    "#     asyncio.run(main())\n",
    "\n",
    "# We'll simply call main() here:\n",
    "await main()\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "run_fills_orders_v2",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
